{"name":"Test - tutorial en cours de conception","tagline":"","body":"# Tutorial en cours d'écriture\r\n\r\nCe tutorial est actuelleent en cours de rédaction et n'est pas encore finalisé.\r\n\r\n\r\n#  Crawler un site internet et obtenir un moteur de recherche\r\n\r\nCe tutorial rapide à réaliser vous permettra de prendre en main certaines fonctionnalités d'OpenSearchServer. Vous apprendez à crawler un site pour construire l'index de recherche, mettre en place une page de recherche, configurer des facettes et activer l'auto-complétion !\r\n\r\nPour commencer ce tutorial il vous suffit simplement d'[Installer OpenSearchServer](http://www.open-search-server.com/fr/tester-opensearchserver).\r\n\r\nNous imaginerons ici que votre site Internet est un site d'actualités. Nous avons créé pour cela 4 pages fictives : XXXXXX\r\n\r\n## Mise en place du crawl et indexation des contenus\r\n\r\n### Création et configuration initiale de l'index\r\n\r\nCommençons par créer un `index`. L'index est le cœur d'OpenSearchServer, c'est autour des index que le reste des fonctionnalités s'organisent. L'index permet de stocker et d'indexer tous les **documents** qui lui sont soumis. La plupart du temps un document correspond à une page web, représentée par son URL, mais cela peut aussi être un fichier ou un contenu issu d'une base de données.\r\n\r\n* Nom de l'index : `site`\r\n* Template : `Empty index`\r\n\r\nCliquez sur `Create`.\r\n\r\nL'index est créé immédiatement. Le contexte global de l'interface change et de nouveaux onglets apparaissent en haut de page.\r\n\r\nSélectionnez l'onglet `Schema`. Le schéma permet de définir quels sont les champs de l'index. \r\nUn champ de schéma possède 5 propriétés :\r\n* **Name** : le nom du champ\r\n* **Indexed** : indique si la valeur du champ doit être indexée, ce qui permettra alors d'effectuer des requêtes dessus. Il arrive que certains champs ne soient pas utilisés dans les recherches mais doivent tout de même être retournés (voir propriété suivante) lors d'une requête de recherche.\r\n* **Stored** : indique si la valeur du champ doit être stockée telle quelle. Cela permettra de renvoyer la donnée brute lors d'une requête de recherche.\r\n* **TermVector** : indique si des `snippets` pourront être configurés sur ce champ. Les snippet sont des extraits de texte contenant les mots recherchés.\r\n* **Analyzer** : les `analyzers` sont des ensembles de filtre et de traitements automatiques qui peuvent être effectués sur les valeurs indexées. Cette fonctionnalité ne sera pas abordée durant ce tutorial. \r\n\r\n\r\n***\r\n**/!\\ Voir pour l'utilisation exacte des analyzer : lesquels configurés sur quels champs...**\r\n***\r\n\r\n\r\n\r\nNous allons créer 4 champs pour indexer nos actualités : url, title, section (rubrique de l'actualité) et content.\r\n\r\nCréez le champ url :\r\n* **Name** : url\r\n* **Indexed** : yes\r\n* **Stored** : yes\r\n* **TermVector** : no\r\n* **Analyzer** : laisser vide\r\n\r\nCliquez sur le bouton `Add`.\r\n\r\nLe champ est ajouté en dessous dans la zone __List of existing fields and their settings__.\r\n\r\nCréez les 3 autres champs en choisissant ces  options :\r\n* title :\r\n  * **Name** : title\r\n  * **Indexed** : yes\r\n  * **Stored** : yes\r\n  * **TermVector** : no\r\n  * **Analyzer** : laisser vide\r\n* section : \r\n  * **Name** : section\r\n  * **Indexed** : yes\r\n  * **Stored** : yes\r\n  * **TermVector** : no\r\n  * **Analyzer** : laisser vide\r\n* content :\r\n  * **Name** : content\r\n  * **Indexed** : yes\r\n  * **Stored** : no\r\n  * **TermVector** : no\r\n  * **Analyzer** : laisser vide\r\n\r\nNos 4 champs sont maintenant créés et visibles dans la liste des champs.\r\n\r\nNous devons configuré un champ par défaut et un champ unique. Pour cela sélectionnez `title` dans la première liste et `url` dans la seconde.\r\n\r\n\r\n### Configuration du crawl\r\n\r\nNous devons maintenant configurer le crawler web d'OSS afin qu'il parcoure et qu'il indexe les pages désirées.\r\n\r\nRendez-vous dans l'onglet `Crawler` de l'index site. La section crawler contient deux sous navigation par onglets. Le premier des ces deux niveaux permet de choisir entre la configuration du crawler web, du crawler de base de données et du crawler de système de fichier.\r\n\r\nRestons sur l'onglet `Web`. Le second niveau de navigation permet de naviguer à travers les rubriques du crawler web.\r\n\r\nL'onglet sélectionné par défaut, `Pattern list`, est celui qui nous intéresse ici.\r\n\r\nLe site que nous souhaitons crawler est [http://xxxxxxxxxxxxxxxxx](http://xxxxxxxxxxxxxxxxx). Nous pouvons voir que cette URL contient les liens vers toutes les pages d'actualités. Nous pouvons donc indiquer au crawler de commencer son crawl ici et d'indexer toutes les pages se trouvant \"sous\" cette URL.\r\n\r\nDans le champ de saisie de l'onglet Pattern list indiquez `http://xxxxxxxxxxxxx/*` puis cliquer sur le bouton Add. L'URL renseigné s'ajoute à la zone du dessous contenant toutes les URL à crawler.\r\n\r\nLa partie `/*` indique ici au crawler de parcourir toutes les pages se trouvant sous l'URL principale.\r\n\r\nRendez-vous ensuite dans l'onglet `Field mapping`. Nous allons ici configurer le crawler pour qu'il place automatiquement l'URL de la page crawlée dans le champ `url` du schéma. Le crawler peut en effet manipuler directement un certain nombre d'éléments issus de la page web crawlée, comme par exemple son url, les headers de réponses, l'url referer, etc.\r\n\r\nChoisissez `url` dans les deux listes déroulantes puis cliquez sur `Add`. Le mapping entre les deux champs s'ajoute immédiatement dans la zone du dessous.\r\n\r\nVoilà, nous avons configurer le crawler en 2 étapes ! Nous devons maintenant terminer la configuration du schéma avant de lancer le crawl.\r\n\r\n\r\n### Configuration finale du schéma\r\n\r\nNous allons voir rapidement comment extraire du contenu de la page HTML pour le placer automatiquement dans les champs du schéma.\r\n\r\nRetournez dans l'onglet `Schema` et cliquez sur l'onglet `Parser list`. Cette page présente les différents `parser` disponibles. Cliquez sur le bouton `Edit` sur la ligne `HTML parser`. La page d'édition du parser HTML s'affiche. Cliquez sur l'onglet `Field mapping`.\r\n\r\n***\r\n**/!\\  Voir pour la liste par défaut : supprimer les parsers existant pour en recréer un HTML simple ?**\r\n\r\n.......finaliser explication regexp.......\r\n* extraction de la rubrique (section) de la page crawlée à partir du fil d’ariane de nos fausses pages avec une regexp\r\n* extraction du title\r\n* extraction du content\r\n\r\n***\r\n\r\n### Démarrage du crawl\r\n\r\nIl est maintenant temps de démarrer le crawler configuré plus tôt. Rendez-vous pour cela dans l'onglet `Crawler` puis `Crawl process`. Différents paramètres liés au crawl peuvent être réglés ici mais les valeurs par défaut sont satisfaisantes pour notre exemple. \r\n\r\nCliquez sur le bouton `Not running - click to run` afin de lancer le crawl. Ici encore, le process s'actualise immédiatement dans la zone du dessous.\r\n\r\nPendant que les pages sont crawlés et les documents ajoutés à l'index nous allons voir comment les recherches peuvent être effectuées dans ces documents.\r\n\r\n## Rechercher les contenus indexés et personnaliser la pertinence des résultats\r\n\r\n### Création de la requête de recherche full-text\r\n\r\nCliquez sur l'onglet `Query`. Dans le champ `Query name` saisissez `search_articles` puis cliquez sur le bouton `New query...`\r\n\r\nLes `query` peuvent ensuite être construites avec un formalisme puissant, mais facilement abordable. Il faut en effet indiquer au moteur de recherche dans quel champ la recherche full-text doit s'effectuer et quel poids accorder à chaque champ. \r\n\r\nEn effet comme nous l'avons vu au début de ce tutorial certains champs peuvent n'être que stockés mais pas indexés, car nous ne souhaitons pas effectuer de recherche dessus.\r\nDe plus nous pouvons considérer que des documents qui contiendront les mots recherchés dans leur titre ont plus de poids, et donc plus de pertinence, que les documents ne contenant ces mots que dans leur contenu.\r\n\r\nNous pouvons donc utiliser cette requête :\r\n\r\n    title:($$)^10 OR title:(\"$$\") OR\r\n    section:($$)^7 OR section:(\"$$\") OR\r\n    content:($$)^4 OR content:(\"$$\")\r\n\r\n`$$` représente ici le ou les mots saisis lors de la recherche.\r\n\r\nNous indiquons au moteur de rechercher d'abord dans le titre avec un poids important, puis ensuite dans la rubrique et enfin dans le contenu de l'article.\r\n\r\nNous utilisons également la notation `($$)` et `(\"$$\")` : cela permet d'obtenir des documents contenant les mots recherchés soit de manière éclatée soit de manière regroupée.\r\n\r\n***\r\n**/!\\ Voir en fonction des analyser ce qu'il faut utiliser exactement au niveau de la recherche**\r\n***\r\n\r\nSaisissez la requête dans le champ `Pattern query` puis cliquez sur le bouton `Save` se trouvant en haut à droite de la page.\r\n\r\nNous pouvons à présent effectuer des recherches sur les documents qui ont été indexés durant le temps de création de la requête.\r\n\r\nCliquez sur le bouton `Edit` de la requête afin de revenir à sa page d'édition. Dans le champ `Enter the query` saisissez `chômage` puis cliquez sur le bouton `Search`. Le moteur retourne les documents correspondant dans la zone du dessous. \r\n\r\n## Proposer une page de recherche aux utilisateurs\r\n\r\nJusqu'à présent nous avons pu rapidement mettre en place un index de document, un crawler de page web et une manière de rechercher les documents.\r\n\r\nNous allons maintenant voir comment mettre à disposition des utilisateurs de notre site internet ce moteur de recherche.\r\n\r\nCliquez sur l'onglet `Renderer` puis sur le bouton `New renderer...`.\r\n\r\nDans le champ `Renderer name` saisissez `default` et sélectionner la requête `search_articles` dans la liste déroulante `Request name`.\r\nCliquer ensuite sur le bouton `Create`.\r\n\r\nNous avons ainsi créer une page de recherche qui utilisera la requête `search_articles` configurée au préalable. \r\n\r\nDans la liste des `renderer` cliquez sur le bouton `View` sur la ligne du renderer `default`. \r\n\r\nLa page obtenue contient un formulaire de recherche directement utilisable par les internautes ! \r\n\r\nVous pouvez retourner dans l'édition du renderer pour définir des CSS personnalisés et modifier les autres paramètres. L'onglet `Testing` vous fournira le code source de l'iFrame à intégrer sur une page de votre site internet.\r\n\r\n## Ajout d’une facette\r\n\r\nNous avons configuré le crawler et le schéma pour qu'à chaque article indexé soit associé une rubrique. Nous allons maintenant voir comment exposer cette rubrique en tant que facette. \r\n\r\nLes facettes sont des compteurs thématiques de résultats, qui servent également de filtre de recherche.\r\n\r\nCliquez sur l'onglet `Query` puis sur le bouton `Edit` de la requête `search_articles`.\r\n\r\nCliquez ensuite sur l'onglet `Faceted fields`, choisissez `section` dans la liste puis cliquez sur le bouton `add facet`. Cliquez ensuite sur le bouton `Save` en haut à droite.\r\n\r\nRé-affichez la page du renderer (clic sur `View` dans l'onglet `renderer`) et effectuez une recherche : la facette est automatiquement ajoutée !\r\n\r\n\r\n## Ajout de l’autocomplétion\r\n\r\n## Que faire ensuite ?\r\n\r\nNous venons de mettre en pratique quelques-unes des très nombreuses fonctionnalités proposées par OpenSearchServer. \r\n\r\nVous pouvez maintenant découvrir le reste de [notre Centre de documentation](http://www.open-search-server.com/confluence/display/EN/Home), qui vous permettra de comprendre les autres paramétrages du moteur.\r\nVous y trouverez également toute la documentation sur l'ensemble des API fournies par OpenSearchServer. L'utilisation de ces API, couplée avec [nos librairies clientes](https://github.com/jaeksoft), vous permettra d'intégrer très facilement et finement le moteur de recherche à votre application.\r\n\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}